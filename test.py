# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EqLR1fFVDWlqtZ2wuRdVWFwjVcrPJPW7

# Script Description: test.py

This script loads the test_loader.pth file (generated if data_loading.py script was executed with the *'use_validation_model'* option set to ***True*** in the config.ini file to make the split) as test dataloader from /app/application_file/data_loaders, then loads the corresponding validation_model.pth file as model to be evaluated from the corresponding folder inisde the /models folder according to the architecture set in the config.ini file (generated if the train.py script was executed with the corresponding architecture and *'use_validation_model'* option set to ***True*** in the config.ini file), evaluates this model using the test dataloader, and prints the validation_loss over the whole test dataset as well as an example image (obtained from this test dataloader) with its corresponding predictions.

## Importing required libraries
"""

import numpy as np
import matplotlib
matplotlib.style.use('ggplot')
import matplotlib.pyplot as plt
import torch
import random
import os
import sys
import configparser

print('\n--> test.py execution starts...\n')

"""## Setting GPU as processing device instead of CPU"""

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""## Initializing config parser and reading config.ini"""

config = configparser.ConfigParser()
config.read('./app/application_files/config.ini')

"""## Loading paths, constants and options from configuration file"""

# Size of images
SIZE = int(config['CONSTANTS']['SIZE'])

#Model architecture
MODEL_ARCHITECTURE = config['CONSTANTS']['MODEL_ARCHITECTURE']

# True if we want to reproduce the same results setting fixed seeds

reproducibility = True if config['OPTIONS']['reproducibility'] == "True" else False

"""## Importing own-built architecture library where our model architectures are defined and own-built library where our customised FacialKeyPointDataset class is defined since is required to load the dataloaders"""

import architecture

#Adding path (where the customised library is located) to the system path so it can be imported
library_path = os.getcwd()+'/app/application_files'
sys.path.append(library_path)

import customized_dataset_augmentation_library

"""## Setting fixed seeds and configurations for reproducibility if option activated"""

if reproducibility:
  
  torch.backends.cudnn.deterministic = True
  random.seed(1)
  torch.manual_seed(1)
  torch.cuda.manual_seed(1)
  np.random.seed(1)

"""# 1) Loading Test DataLoader """

print('Loading test dataloader ...\n')

try:
    test_loader = torch.load('./app/application_files/dataloaders/test_loader.pth')
except FileNotFoundError:
    raise RuntimeError('No test_loader.pth in /app/application_files/dataloaders folder. It has not been generated yet. Run the data_loading.py script with the corresponding config settings to generate it.')

print('test dataloader loaded\n')

"""# 2) Loading validation model"""

print('Loading validation model ...\n')

import torch.nn as nn

if MODEL_ARCHITECTURE == "RESNET":
  model = architecture.FaceKeypointModelResNet().to(DEVICE)
elif MODEL_ARCHITECTURE == "MOBILENET":
  model = architecture.FaceKeypointModelMobileNet().to(DEVICE)
elif MODEL_ARCHITECTURE == "VGG":
  model = architecture.FaceKeypointModelVGG().to(DEVICE)
else:
  raise RuntimeError('No valid value for model architecture in config.ini file')

#Load model state after training
try:
    save = torch.load('./models/'+MODEL_ARCHITECTURE+'/validation_model.pth')
except FileNotFoundError:
    raise RuntimeError('No validation_model.pth model in /models/'+MODEL_ARCHITECTURE+ ' folder. It has not been generated yet. Run the train.py script with the corresponding config settings to generate it.')

model.load_state_dict(save.get('model_state_dict')) 
model.to(DEVICE)
# we need a loss function which is good for regression like MSELoss
criterion = nn.MSELoss()

print('validation model loaded\n')

"""# 3) Evaluate the validation_model"""

def validation(model, val_dataloader):
    model.eval()
    valid_running_loss = 0.0
    counter = 0

    # calculate the number of batches
    num_batches = int(len(val_dataloader.dataset)/val_dataloader.batch_size)
    with torch.no_grad():
        #for i, data in tqdm(enumerate(val_dataloader), total=num_batches):
        for data in val_dataloader:
            counter += 1
            image, keypoints = data['image'].to(DEVICE), data['keypoints'].to(DEVICE)
            # flatten the keypoints
            keypoints = keypoints.view(keypoints.size(0), -1)
            outputs = model(image)
            loss = criterion(outputs, keypoints)
            valid_running_loss += loss.item()
            #storing last batch and outputs for the "Actual vs Predicted" Keypoints Comparison Plotting
            last_batch=data
            last_batch_outputs=outputs

    valid_loss = valid_running_loss/counter

    

    #Getting image, actual keypoints and predicted output keypoints for the first sample of the last batch used for the "Actual vs Predicted" Keypoints Comparison Plotting
    first_sample_image_in_last_val_batch, first_sample_actual_keypoints_in_last_val_batch, first_sample_predcited_keypoints_in_last_val_batch = last_batch['image'][0], last_batch['keypoints'][0], last_batch_outputs[0].cpu()

    print(f'Val Loss: {valid_loss:.4f}')
    print(f'Actual (red) vs Predictive (blue) Keypoints Comparison Plotting for the first sample of the last validation batch:')
    plt.clf()# clean plot
    plt.imshow(first_sample_image_in_last_val_batch.reshape(SIZE, SIZE), cmap='gray')
    plt.plot(first_sample_actual_keypoints_in_last_val_batch[:,0], first_sample_actual_keypoints_in_last_val_batch[:,1], 'r.')
    plt.plot(first_sample_predcited_keypoints_in_last_val_batch[::2], first_sample_predcited_keypoints_in_last_val_batch[1::2], 'b.')
    plt.show()

print('MODEL EVALUATION STARTS:\n')

validation(model, test_loader)

print('\nMODEL EVALUATION FINISHED\n')

print('\n--> test.py execution finished')