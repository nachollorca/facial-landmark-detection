# -*- coding: utf-8 -*-
"""architecture.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q2kglSI6NW2Q-sYEV-3BHHwTQt9gflEj

# Script Description: architecture.py

This script defines the three CNN model architectures currently available in the project to build the models depending on the configurations set in the config.ini file (located inside the /app/appplication_files folder). More detailed analysis for each architecture can be found in the report. Each architecture is based on one of these three following CNN architecture structures:


*   **ResNet**
*   **MobileNet**
*   **VGG** (option providing the best results out of the three. The result analysis can be found inside the report)

## Importing required libraries
"""

import torch.nn as nn
import torch.nn.functional as F

"""# ResNet Model Architecture"""

# Residual Block from Dive Into Deep Learning 
# (http://d2l.ai/chapter_convolutional-modern/resnet.html)

class Residual(nn.Module):  
    """The Residual block of ResNet."""
    def __init__(self, input_channels, num_channels, use_1x1conv=False,
                 strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3,
                               padding=1, stride=strides)
        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3,
                               padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels,
                                   kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)

    def forward(self, X):
        Y = self.conv1(X)
        Y = self.bn1(Y)
        Y = F.relu(Y)
        Y = self.conv2(Y)
        Y = self.bn2(Y)
        if self.conv3:
            X = self.conv3(X)
        Y += X

        return F.relu(Y)

# Define model architecture
class FaceKeypointModelResNet(nn.Module):
    def __init__(self):
        super(FaceKeypointModelResNet, self).__init__()

        self.conv = nn.Conv2d(1, 32, kernel_size=3, padding = 'same')
        self.pool = nn.MaxPool2d(2, 2)

        self.blk1 = Residual(32, 64, use_1x1conv=True, strides=2)
        self.blk2 = Residual(64, 128, use_1x1conv=True, strides=2)
        self.blk3 = Residual(128, 256, use_1x1conv=True, strides=2)
        self.blk4 = Residual(256, 512, use_1x1conv=True, strides=2)

        self.fc1 = nn.Linear(512, 256) 
        self.fc2 = nn.Linear(256,30)
    def forward(self, x):
         x = self.conv(x)
         x = self.pool(x)
         x = self.blk1(x)
         x = self.blk2(x)
         x = self.blk3(x)
         x = self.blk4(x)

         # bs is the batch size or respectively the number of instances 
         # simultaneously loaded into the model
         bs, _, _, _ = x.shape 

         # The next step finally transforms the images into 1-dim vectors of lenght 128
         x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)

         # apply a classic linear function
         x = self.fc1(x)
         out = self.fc2(x) 

         return out

"""# VGG Model Architecture"""

# Define model architecture
class FaceKeypointModelVGG (nn.Module):
    def __init__(self):
        super(FaceKeypointModelVGG, self).__init__()

        # modle adapted from the concept of VGG net and facial-keypoint-detection.ipynb
        self.conv1 = nn.Conv2d(1, 32, kernel_size = 3, padding = 2)
        self.batch_normal1 = nn.BatchNorm2d(32)
        
        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding =2 ) 
        self.batch_normal2 = nn.BatchNorm2d(32)

        self.conv3= nn.Conv2d(32, 64, kernel_size = 3, padding = 2)
        self.batch_normal3 = nn.BatchNorm2d(64)
         
        self.conv4= nn.Conv2d(64, 64, kernel_size = 3, padding = 2)
        self.batch_normal4 = nn.BatchNorm2d(64)

        self.conv5= nn.Conv2d(64, 96, kernel_size = 3, padding = 2)
        self.batch_normal5 = nn.BatchNorm2d(96)

        self.conv6= nn.Conv2d(96, 96, kernel_size = 3, padding = 2)
        self.batch_normal6 = nn.BatchNorm2d(96)

        self.conv7= nn.Conv2d(96, 128, kernel_size = 3, padding = 2)
        self.batch_normal7 = nn.BatchNorm2d(128)

        self.conv8= nn.Conv2d(128, 128, kernel_size = 3, padding = 2)
        self.batch_normal8 = nn.BatchNorm2d(128)

        self.conv9= nn.Conv2d(128, 256, kernel_size = 3, padding = 2)
        self.batch_normal9 = nn.BatchNorm2d(256)

        self.conv10= nn.Conv2d(256, 256, kernel_size = 3, padding = 2)
        self.batch_normal10 = nn.BatchNorm2d(256) 

        self.conv11= nn.Conv2d(256, 512, kernel_size = 3, padding = 2)
        self.batch_normal11 = nn.BatchNorm2d(512)

        self.conv12= nn.Conv2d(512, 512, kernel_size = 3, padding = 2)
        self.batch_normal12 = nn.BatchNorm2d(512)  

        self.fc1 = nn.Linear(512, 512)
        self.fc2 = nn.Linear(512, 30 )
        self.pool = nn.MaxPool2d(2, 2)
        self.activ = nn.LeakyReLU(0.1)
        self.dropout = nn.Dropout2d(p=0.2)
        
    def forward(self, x):
         x = self.conv1(x)
         x = self.activ(x)
         x = self.batch_normal1(x)

         x = self.conv2(x)
         x = self.activ(x)
         x = self.batch_normal2(x)
         x = self.pool(x)

         x = self.conv3(x)
         x = self.activ(x)
         x = self.batch_normal3(x)

         x = self.conv4(x)
         x = self.activ(x)
         x = self.batch_normal4(x)
         x = self.pool(x)

         x = self.conv5(x)
         x = self.activ(x)
         x = self.batch_normal5(x)

         x = self.conv6(x)
         x = self.activ(x)
         x = self.batch_normal6(x)
         x = self.pool(x)

         x = self.conv7(x)
         x = self.activ(x)
         x = self.batch_normal7(x)

         x = self.conv8(x)
         x = self.activ(x)
         x = self.batch_normal8(x)
         x = self.pool(x)

         x = self.conv9(x)
         x = self.activ(x)
         x = self.batch_normal9(x)

         x = self.conv10(x)
         x = self.activ(x)
         x = self.batch_normal10(x)
         x = self.pool(x)

         x = self.conv11(x)
         x = self.activ(x)
         x = self.batch_normal11(x)

         x = self.conv12(x)
         x = self.activ(x)
         x = self.batch_normal12(x)

        
      

         # bs is the batch size or respectively the number of instances 
         # simultaneously loaded into the model
         bs, _, _, _ = x.shape 

         # The next step finally transforms the images into 1-dim vectors of lenght 128
         x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)
         # Apply dropout for regularization and preventing the co-adaptation of neurons
         # https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html?highlight=dropout#torch.nn.Dropout
         x = self.dropout(x)
         # apply a classic linear function
         x = self.fc1(x)
         out = self.fc2(x) 
         return out

"""# MobileNet Model Architecture"""

# Define model architecture
class FaceKeypointModelMobileNet (nn.Module):
    def __init__(self):
        super(FaceKeypointModelMobileNet, self).__init__()

        # model adapted from the concept of VGG net and facial-keypoint-detection.ipynb
        # Implementing MobileNet concept to improve the runtime. when implementing the MobileNet
        # two layers of previous have been included in one depthwise seperable convolution.
        self.conv1 = nn.Conv2d(1, 1, kernel_size = 3, padding = 2, groups = 1)
        self.conv1_1= nn.Conv2d(1, 32, kernel_size = 1)
        self.batch_normal1 = nn.BatchNorm2d(32)

        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding = 2, groups = 32 )
        self.conv2_2= nn.Conv2d(32, 64, kernel_size = 1)
        self.batch_normal2 = nn.BatchNorm2d(64)
         
        self.conv3= nn.Conv2d(64, 64, kernel_size = 3, padding = 2, groups = 64)
        self.conv3_3= nn.Conv2d(64, 96, kernel_size = 1)
        self.batch_normal3 = nn.BatchNorm2d(96)

        self.conv4 = nn.Conv2d(96, 96, kernel_size = 3, padding = 2, groups = 96)
        self.conv4_4 = nn.Conv2d(96, 128, kernel_size = 1)
        self.batch_normal4 = nn.BatchNorm2d(128)

        self.conv5= nn.Conv2d(128, 128, kernel_size = 3, padding = 2, groups = 128)
        self.conv5_5 = nn.Conv2d(128, 256, kernel_size = 1)
        self.batch_normal5 = nn.BatchNorm2d(256)

        self.conv6= nn.Conv2d(256, 256, kernel_size = 3, padding = 2, groups = 256)
        self.conv6_6= nn.Conv2d(256, 512, kernel_size = 1)
        self.batch_normal6 = nn.BatchNorm2d(512)  

        self.fc1 = nn.Linear(512, 512)
        self.fc2 = nn.Linear(512, 30 )
        self.pool = nn.MaxPool2d(2, 2)
        self.activ = nn.LeakyReLU(0.1)
        self.dropout = nn.Dropout2d(p=0.2)
 
    def forward(self, x):
         x = self.conv1(x)
         x = self.conv1_1(x)
         x = self.activ(x)
         x = self.batch_normal1(x)
         x = self.pool(x)

         x = self.conv2(x)
         x = self.conv2_2(x)
         x = self.activ(x)
         x = self.batch_normal2(x)
         x = self.pool(x)
         
         x = self.conv3(x)
         x = self.conv3_3(x)
         x = self.activ(x)
         x = self.batch_normal3(x)
         x = self.pool(x)

         x = self.conv4(x)
         x = self.conv4_4(x)
         x = self.activ(x)
         x = self.batch_normal4(x)
         x = self.pool(x)

         x = self.conv5(x)
         x = self.conv5_5(x)
         x = self.activ(x)
         x = self.batch_normal5(x)
         x = self.pool(x)

         x = self.conv6(x)
         x = self.conv6_6(x)
         x = self.activ(x)
         x = self.batch_normal6(x)
         x = self.pool(x)
      

         # bs is the batch size or respectively the number of instances 
         # simultaneously loaded into the model
         bs, _, _, _ = x.shape 

         # The next step finally transforms the images into 1-dim vectors of lenght 128
         x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)
         # Apply dropout for regularization and preventing the co-adaptation of neurons
         # https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html?highlight=dropout#torch.nn.Dropout
         x = self.dropout(x)
         # apply a classic linear function
         x = self.fc1(x)
         out = self.fc2(x) 

         return out