[PATHS]
# It specifies to the data_loading.py script the relative path to itself where the training.csv file is located. The data_loading.py script will read this file and use its data to generate the corresponding customized datasets and dataloaders needed by the others scripts. Its curent value (../Data/training.csv) is the one used by the project group, where we had stored this csv file. So the user needs to change it beforehand in case of running the data_loading.py script.
training_csv_path_relative_to_scripts_directory = ../Data/training.csv 

[CONSTANTS]
# It specifies the model architecture used to build and train the model. Only three possible values [RESNET, VGG , MOBILENET] to select one of the three available architectures.
MODEL_ARCHITECTURE = VGG 
# It specifies the batch size used in the model training done in the train.py script, as well for the evaluation in the test.py script.
BATCH_SIZE = 256 
# It specifies the learning rate in the model training done in the train.py script.
LR = 0.0001
# It specifies the number of epochs used in the model training done in the train.py script.
EPOCHS= 400
# It specifies the percentage (0.2 -> 20% test & 80% training) used in the data_loading.py script to split the input training.csv file and create both training and test datasets and their corresponding dataloaders that both train.py and test.py scripts will use to train and evaluate the model. (if use_validation_model option is set to True)
TEST_SPLIT= 0.2
# It specifies the size of the image. Its value is used across all the scripts.
SIZE= 96 

[OPTIONS]
# If 'use_validation_model' is set to True, the data_loading.py will split the input training.csv file and create both training and test datasets and their corresponding dataloaders (storing these last ones in /app/application_file/data_loaders as training_loader.pth and test_loader.pth). The train.py script will use the training_loader.pth datalaoder in /app/application_file/data_loaders to train the model and saving it in /models/MODEL_ARCHITECTURE folder (depending on the architecture used) as validation_model.pth if the option is activated. If 'use_validation_model' is set to False, the data_loading.py will not split the input training.csv file and just create one dataset and its corresponding dataloader with the whole data (storing this in /app/application_file/data_loaders as all_data_loader.pth). The train.py will use the all_data_loader.pth datalaoder in /app/application_file/data_loaders to train the model and saving it in /models/MODEL_ARCHITECTURE folder (depending on the architecture used) as final_model.pth.
use_validation_model = False
# If 'generate_model' is set to True, when we execute the demo.colab file, it will run the data_loading.py script to create the corresponding dataloaders (or overwrite if they alredy exist), then run the train.py script to train the model with the corresponding dataloader and saving it in /models/MODEL_ARCHITECTURE folder (depending on the architecture used), and, if the use_validation_model is activated, run the test.py script using the test dataloader to evaluate the validation model.
generate_model = False
# If 'show_images_during_training' is set to True, the train.py script plots an example image containing their labels and predictions of the model for different epochs so we can see how well the model predicts in each of these epochs. Useful to see the training evolution. Otherwise, no image plot.
show_images_during_training = True
# If 'reproducibility' is set to True, it forces reproducibilty in the scripts data_loading.py, train.py and test.py.
reproducibility = True 